{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:26:43.741054Z",
     "start_time": "2018-05-19T21:26:41.118787Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import humanize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:26:45.592748Z",
     "start_time": "2018-05-19T21:26:43.747634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts:\n",
      "\ttrain: 628906\n",
      "\ttest: 55547\n"
     ]
    }
   ],
   "source": [
    "X_train = pickle.load(open('./data/stage4-train.pkl', 'rb'))\n",
    "y_train = X_train.pop('stop_outcome')\n",
    "X_test = pickle.load(open('./data/stage4-test.pkl', 'rb'))\n",
    "y_test = X_test.pop('stop_outcome')\n",
    "\n",
    "print('Row counts:\\n\\ttrain: {}\\n\\ttest: {}'.format(X_train.shape[0], X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:26:45.799126Z",
     "start_time": "2018-05-19T21:26:45.601391Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, PassiveAggressiveClassifier, Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier as EnsExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "# clf1 = RandomForestClassifier(n_jobs=8, verbose=3, random_state=0)\n",
    "# clf2 = GaussianNB()\n",
    "# clf3 = DecisionTreeClassifier(random_state=0)\n",
    "# clf4 = GradientBoostingClassifier(verbose=3, random_state=0)\n",
    "\n",
    "# eclf = VotingClassifier(estimators=[('rf', clf1), ('gnb', clf2), ('dt', clf3), ('gb', clf4)],\n",
    "#                         voting='soft')\n",
    "\n",
    "clf5 = BernoulliNB()\n",
    "clf6 = ExtraTreeClassifier()\n",
    "clf7 = EnsExtraTreesClassifier(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf8 = KNeighborsClassifier(n_jobs=-1)\n",
    "clf9 = LogisticRegressionCV(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf10 = MLPClassifier(verbose=3, random_state=0)\n",
    "# clf11 = LinearSVC(verbose=3, random_state=0)\n",
    "clf12 = Perceptron(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf13 = PassiveAggressiveClassifier(n_jobs=-1, verbose=3, random_state=0)\n",
    "# clf14 = ()\n",
    "# clf15 = ()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[\n",
    "                            ('bnb', clf5),\n",
    "                            ('etc', clf6),\n",
    "                            ('eetc', clf7),\n",
    "                            ('knc', clf8),\n",
    "                            ('lrcv', clf9),\n",
    "                            ('mlpc', clf10),\n",
    "#                             ('lsvc', clf11),\n",
    "                            ('p', clf12),\n",
    "                            ('pac', clf13),\n",
    "#                             ('', clf14),\n",
    "#                             ('', clf15),\n",
    "                        ], voting='soft')\n",
    "\n",
    "# with parallel_backend('threading'):\n",
    "#     eclf = eclf.fit(X_train, y_train)\n",
    "# print('eclf score: {}'.format(eclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:26:56.521599Z",
     "start_time": "2018-05-19T21:26:45.808275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf5 score: 0.4708805155994023\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf5.fit(X_train, y_train)\n",
    "print('clf5 score: {}'.format(clf5.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:27:04.954947Z",
     "start_time": "2018-05-19T21:26:56.533986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf6 score: 0.5338542135488865\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf6.fit(X_train, y_train)\n",
    "print('clf6 score: {}'.format(clf6.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:27:20.855862Z",
     "start_time": "2018-05-19T21:27:04.956674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10building tree 2 of 10building tree 3 of 10building tree 4 of 10building tree 5 of 10building tree 6 of 10building tree 7 of 10building tree 8 of 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    6.2s remaining:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    6.7s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=8)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done   7 out of  10 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf7 score: 0.601022557473851\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf7.fit(X_train, y_train)\n",
    "print('clf7 score: {}'.format(clf7.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:28:25.127228Z",
     "start_time": "2018-05-19T21:27:20.864817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf8 score: 0.40045366986515923\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf8.fit(X_train, y_train)\n",
    "print('clf8 score: {}'.format(clf8.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:33:36.620180Z",
     "start_time": "2018-05-19T21:28:25.128948Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:  2.9min remaining:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  4.6min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.7min finished\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf9 score: 0.4304102831836103\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf9.fit(X_train, y_train)\n",
    "print('clf9 score: {}'.format(clf9.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:42:06.878778Z",
     "start_time": "2018-05-19T21:33:36.621156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.31918717\n",
      "Iteration 2, loss = 1.24864087\n",
      "Iteration 3, loss = 1.23808084\n",
      "Iteration 4, loss = 1.23015632\n",
      "Iteration 5, loss = 1.22531467\n",
      "Iteration 6, loss = 1.22133099\n",
      "Iteration 7, loss = 1.21803742\n",
      "Iteration 8, loss = 1.21472453\n",
      "Iteration 9, loss = 1.21231463\n",
      "Iteration 10, loss = 1.20915888\n",
      "Iteration 11, loss = 1.20723513\n",
      "Iteration 12, loss = 1.20469523\n",
      "Iteration 13, loss = 1.20216142\n",
      "Iteration 14, loss = 1.20065936\n",
      "Iteration 15, loss = 1.19867182\n",
      "Iteration 16, loss = 1.19728568\n",
      "Iteration 17, loss = 1.19581997\n",
      "Iteration 18, loss = 1.19428161\n",
      "Iteration 19, loss = 1.19282877\n",
      "Iteration 20, loss = 1.19176190\n",
      "Iteration 21, loss = 1.19041936\n",
      "Iteration 22, loss = 1.18980841\n",
      "Iteration 23, loss = 1.18862080\n",
      "Iteration 24, loss = 1.18791335\n",
      "Iteration 25, loss = 1.18731106\n",
      "Iteration 26, loss = 1.18608222\n",
      "Iteration 27, loss = 1.18591219\n",
      "Iteration 28, loss = 1.18508992\n",
      "Iteration 29, loss = 1.18461775\n",
      "Iteration 30, loss = 1.18403898\n",
      "Iteration 31, loss = 1.18324906\n",
      "Iteration 32, loss = 1.18277757\n",
      "Iteration 33, loss = 1.18236487\n",
      "Iteration 34, loss = 1.18159832\n",
      "Iteration 35, loss = 1.18159589\n",
      "Iteration 36, loss = 1.18091627\n",
      "Iteration 37, loss = 1.18077045\n",
      "Iteration 38, loss = 1.18051038\n",
      "Iteration 39, loss = 1.17993035\n",
      "Iteration 40, loss = 1.17966108\n",
      "Iteration 41, loss = 1.17926535\n",
      "Iteration 42, loss = 1.17908971\n",
      "Iteration 43, loss = 1.17850649\n",
      "Iteration 44, loss = 1.17838283\n",
      "Iteration 45, loss = 1.17831245\n",
      "Iteration 46, loss = 1.17800778\n",
      "Iteration 47, loss = 1.17756775\n",
      "Iteration 48, loss = 1.17764871\n",
      "Iteration 49, loss = 1.17724836\n",
      "Iteration 50, loss = 1.17695169\n",
      "Iteration 51, loss = 1.17676578\n",
      "Iteration 52, loss = 1.17663936\n",
      "Iteration 53, loss = 1.17634835\n",
      "Iteration 54, loss = 1.17614623\n",
      "Iteration 55, loss = 1.17633021\n",
      "Iteration 56, loss = 1.17568172\n",
      "Iteration 57, loss = 1.17561479\n",
      "Iteration 58, loss = 1.17531728\n",
      "Iteration 59, loss = 1.17510161\n",
      "Iteration 60, loss = 1.17490035\n",
      "Iteration 61, loss = 1.17485301\n",
      "Iteration 62, loss = 1.17467114\n",
      "Iteration 63, loss = 1.17456046\n",
      "Iteration 64, loss = 1.17436457\n",
      "Iteration 65, loss = 1.17416236\n",
      "Iteration 66, loss = 1.17392637\n",
      "Iteration 67, loss = 1.17382150\n",
      "Iteration 68, loss = 1.17337346\n",
      "Iteration 69, loss = 1.17356481\n",
      "Iteration 70, loss = 1.17334946\n",
      "Iteration 71, loss = 1.17335760\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "clf10 score: 0.5323239778925954\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf10.fit(X_train, y_train)\n",
    "print('clf10 score: {}'.format(clf10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:42:08.406133Z",
     "start_time": "2018-05-19T21:42:06.879586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 5591.34, NNZs: 25, Bias: -2689.000000, T: 628906, Avg. loss: 947.261715\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8593.06, NNZs: 25, Bias: -762.000000, T: 628906, Avg. loss: 2004.285262\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8287.13, NNZs: 25, Bias: 931.000000, T: 628906, Avg. loss: 2043.027622\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8444.58, NNZs: 25, Bias: -2444.000000, T: 628906, Avg. loss: 2056.228327\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10244.64, NNZs: 25, Bias: 54.000000, T: 628906, Avg. loss: 1260.365125\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6805.31, NNZs: 25, Bias: -4008.000000, T: 1257812, Avg. loss: 936.037143\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10427.75, NNZs: 25, Bias: -1024.000000, T: 1257812, Avg. loss: 1992.836282\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10770.49, NNZs: 25, Bias: 918.000000, T: 1257812, Avg. loss: 2018.871392\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9856.37, NNZs: 25, Bias: -3094.000000, T: 1257812, Avg. loss: 2055.071834\n",
      "Total training time: 0.24 seconds.Norm: 12580.66, NNZs: 25, Bias: 13.000000, T: 1257812, Avg. loss: 1208.130780\n",
      "\n",
      "-- Epoch 3Total training time: 0.25 seconds.\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 7252.18, NNZs: 25, Bias: -4654.000000, T: 1886718, Avg. loss: 930.759137\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11253.08, NNZs: 25, Bias: -1140.000000, T: 1886718, Avg. loss: 2004.552390\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 12229.76, NNZs: 25, Bias: 826.000000, T: 1886718, Avg. loss: 2027.243156\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10670.70, NNZs: 25, Bias: -3444.000000, T: 1886718, Avg. loss: 2049.299396\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13795.10, NNZs: 25, Bias: -6.000000, T: 1886718, Avg. loss: 1213.987827\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7524.91, NNZs: 25, Bias: -5075.000000, T: 2515624, Avg. loss: 934.454716\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11661.12, NNZs: 25, Bias: -1181.000000, T: 2515624, Avg. loss: 2005.712089\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 13367.94, NNZs: 25, Bias: 793.000000, T: 2515624, Avg. loss: 2027.141147\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11154.08, NNZs: 25, Bias: -3534.000000, T: 2515624, Avg. loss: 2057.168755\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14497.16, NNZs: 25, Bias: 97.000000, T: 2515624, Avg. loss: 1210.975483\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7743.60, NNZs: 25, Bias: -5230.000000, T: 3144530, Avg. loss: 931.626704\n",
      "Total training time: 0.53 seconds.\n",
      "Norm: 11958.62, NNZs: 25, Bias: -1255.000000, T: 3144530, Avg. loss: 2005.138661\n",
      "Total training time: 0.55 seconds.\n",
      "Norm: 14137.14, NNZs: 25, Bias: 687.000000, T: 3144530, Avg. loss: 2033.490435\n",
      "Total training time: 0.56 seconds.\n",
      "Norm: 11611.06, NNZs: 25, Bias: -3555.000000, T: 3144530, Avg. loss: 2055.876132\n",
      "Total training time: 0.58 seconds.\n",
      "Norm: 15053.23, NNZs: 25, Bias: 9.000000, T: 3144530, Avg. loss: 1209.824843\n",
      "Total training time: 0.58 seconds.\n",
      "clf12 score: 0.6655624966244802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.6s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf12.fit(X_train, y_train)\n",
    "print('clf12 score: {}'.format(clf12.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:42:10.004427Z",
     "start_time": "2018-05-19T21:42:08.407240Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 2.05, NNZs: 25, Bias: -0.605766, T: 628906, Avg. loss: 0.717346Norm: 1.19, NNZs: 25, Bias: -1.180098, T: 628906, Avg. loss: 0.537748\n",
      "\n",
      "Total training time: 0.11 seconds.Total training time: 0.11 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n",
      "Norm: 2.45, NNZs: 25, Bias: -0.175653, T: 628906, Avg. loss: 1.170629\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.20, NNZs: 25, Bias: -0.529956, T: 628906, Avg. loss: 1.219990\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.87, NNZs: 25, Bias: -0.377738, T: 628906, Avg. loss: 1.190482\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.17, NNZs: 25, Bias: -1.156133, T: 1257812, Avg. loss: 0.533993\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.17, NNZs: 25, Bias: -0.533575, T: 1257812, Avg. loss: 0.712417\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.73, NNZs: 25, Bias: -0.268313, T: 1257812, Avg. loss: 1.168268\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.59, NNZs: 25, Bias: -0.413929, T: 1257812, Avg. loss: 1.220447\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.00, NNZs: 25, Bias: -0.412380, T: 1257812, Avg. loss: 1.187698\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.34, NNZs: 25, Bias: -1.225792, T: 1886718, Avg. loss: 0.534466\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.27, NNZs: 25, Bias: -0.632054, T: 1886718, Avg. loss: 0.711583\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.99, NNZs: 25, Bias: -0.216362, T: 1886718, Avg. loss: 1.166697\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.91, NNZs: 25, Bias: -0.417344, T: 1886718, Avg. loss: 1.215588\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.28, NNZs: 25, Bias: -1.203290, T: 2515624, Avg. loss: 0.533425\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.13, NNZs: 25, Bias: -0.365032, T: 1886718, Avg. loss: 1.188518\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.36, NNZs: 25, Bias: -0.651120, T: 2515624, Avg. loss: 0.710692\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.15, NNZs: 25, Bias: -0.331253, T: 2515624, Avg. loss: 1.170256\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.27, NNZs: 25, Bias: -1.203043, T: 3144530, Avg. loss: 0.533784\n",
      "Total training time: 0.56 seconds.\n",
      "Norm: 3.06, NNZs: 25, Bias: -0.459745, T: 2515624, Avg. loss: 1.216034\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.31, NNZs: 25, Bias: -0.699576, T: 3144530, Avg. loss: 0.712852\n",
      "Total training time: 0.60 seconds.\n",
      "Norm: 2.21, NNZs: 25, Bias: -0.282477, T: 2515624, Avg. loss: 1.186827\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.31, NNZs: 25, Bias: -0.205082, T: 3144530, Avg. loss: 1.166698\n",
      "Total training time: 0.64 seconds.\n",
      "Norm: 3.21, NNZs: 25, Bias: -0.401537, T: 3144530, Avg. loss: 1.217880\n",
      "Total training time: 0.71 seconds.\n",
      "Norm: 2.19, NNZs: 25, Bias: -0.332683, T: 3144530, Avg. loss: 1.185929\n",
      "Total training time: 0.72 seconds.\n",
      "clf13 score: 0.6748519271967883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.7s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf13.fit(X_train, y_train)\n",
    "print('clf13 score: {}'.format(clf13.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
