{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:07:19.375851Z",
     "start_time": "2018-05-19T21:07:16.163301Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import humanize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:07:20.418378Z",
     "start_time": "2018-05-19T21:07:19.377991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts:\n",
      "\ttrain: 222186\n",
      "\ttest: 55547\n"
     ]
    }
   ],
   "source": [
    "X_train = pickle.load(open('./data/stage3-train.pkl', 'rb'))\n",
    "y_train = X_train.pop('stop_outcome')\n",
    "X_test = pickle.load(open('./data/stage3-test.pkl', 'rb'))\n",
    "y_test = X_test.pop('stop_outcome')\n",
    "\n",
    "print('Row counts:\\n\\ttrain: {}\\n\\ttest: {}'.format(X_train.shape[0], X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:07:20.714403Z",
     "start_time": "2018-05-19T21:07:20.428187Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, PassiveAggressiveClassifier, Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier as EnsExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "# clf1 = RandomForestClassifier(n_jobs=8, verbose=3, random_state=0)\n",
    "# clf2 = GaussianNB()\n",
    "# clf3 = DecisionTreeClassifier(random_state=0)\n",
    "# clf4 = GradientBoostingClassifier(verbose=3, random_state=0)\n",
    "\n",
    "# eclf = VotingClassifier(estimators=[('rf', clf1), ('gnb', clf2), ('dt', clf3), ('gb', clf4)],\n",
    "#                         voting='soft')\n",
    "\n",
    "clf5 = BernoulliNB()\n",
    "clf6 = ExtraTreeClassifier()\n",
    "clf7 = EnsExtraTreesClassifier(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf8 = KNeighborsClassifier(n_jobs=-1)\n",
    "clf9 = LogisticRegressionCV(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf10 = MLPClassifier(verbose=3, random_state=0)\n",
    "# clf11 = LinearSVC(verbose=3, random_state=0)\n",
    "clf12 = Perceptron(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf13 = PassiveAggressiveClassifier(n_jobs=-1, verbose=3, random_state=0)\n",
    "# clf14 = ()\n",
    "# clf15 = ()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[\n",
    "                            ('bnb', clf5),\n",
    "                            ('etc', clf6),\n",
    "                            ('eetc', clf7),\n",
    "                            ('knc', clf8),\n",
    "                            ('lrcv', clf9),\n",
    "                            ('mlpc', clf10),\n",
    "#                             ('lsvc', clf11),\n",
    "                            ('p', clf12),\n",
    "                            ('pac', clf13),\n",
    "#                             ('', clf14),\n",
    "#                             ('', clf15),\n",
    "                        ], voting='soft')\n",
    "\n",
    "# with parallel_backend('threading'):\n",
    "#     eclf = eclf.fit(X_train, y_train)\n",
    "# print('eclf score: {}'.format(eclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:07:23.876947Z",
     "start_time": "2018-05-19T21:07:20.723467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf5 score: 0.6942409131006175\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf5.fit(X_train, y_train)\n",
    "print('clf5 score: {}'.format(clf5.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:07:26.797317Z",
     "start_time": "2018-05-19T21:07:23.888974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf6 score: 0.5443858354186545\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf6.fit(X_train, y_train)\n",
    "print('clf6 score: {}'.format(clf6.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:07:32.707532Z",
     "start_time": "2018-05-19T21:07:26.808194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10building tree 2 of 10building tree 3 of 10building tree 4 of 10building tree 5 of 10building tree 6 of 10building tree 7 of 10building tree 8 of 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    2.5s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    2.6s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=8)]: Done   3 out of  10 | elapsed:    0.3s remaining:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done   7 out of  10 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf7 score: 0.6085837218931716\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf7.fit(X_train, y_train)\n",
    "print('clf7 score: {}'.format(clf7.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:07:45.076112Z",
     "start_time": "2018-05-19T21:07:32.717427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf8 score: 0.6650764217689524\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf8.fit(X_train, y_train)\n",
    "print('clf8 score: {}'.format(clf8.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:12:28.172220Z",
     "start_time": "2018-05-19T21:07:45.085938Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:  2.1min remaining:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  3.6min remaining:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  3.7min finished\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf9 score: 0.6908023835670694\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf9.fit(X_train, y_train)\n",
    "print('clf9 score: {}'.format(clf9.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:25:23.816969Z",
     "start_time": "2018-05-19T21:12:28.183467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.99783483\n",
      "Iteration 2, loss = 0.88063053\n",
      "Iteration 3, loss = 0.88002316\n",
      "Iteration 4, loss = 0.87244419\n",
      "Iteration 5, loss = 0.86967014\n",
      "Iteration 6, loss = 0.86751917\n",
      "Iteration 7, loss = 0.86755236\n",
      "Iteration 8, loss = 0.86562002\n",
      "Iteration 9, loss = 0.86339248\n",
      "Iteration 10, loss = 0.86259002\n",
      "Iteration 11, loss = 0.86124014\n",
      "Iteration 12, loss = 0.86028589\n",
      "Iteration 13, loss = 0.85920202\n",
      "Iteration 14, loss = 0.85851038\n",
      "Iteration 15, loss = 0.85768835\n",
      "Iteration 16, loss = 0.85671764\n",
      "Iteration 17, loss = 0.85532448\n",
      "Iteration 18, loss = 0.85510836\n",
      "Iteration 19, loss = 0.85405502\n",
      "Iteration 20, loss = 0.85258175\n",
      "Iteration 21, loss = 0.85218395\n",
      "Iteration 22, loss = 0.85147813\n",
      "Iteration 23, loss = 0.84997986\n",
      "Iteration 24, loss = 0.84976569\n",
      "Iteration 25, loss = 0.84931089\n",
      "Iteration 26, loss = 0.84867824\n",
      "Iteration 27, loss = 0.84836087\n",
      "Iteration 28, loss = 0.84722988\n",
      "Iteration 29, loss = 0.84703787\n",
      "Iteration 30, loss = 0.84634585\n",
      "Iteration 31, loss = 0.84637480\n",
      "Iteration 32, loss = 0.84469729\n",
      "Iteration 33, loss = 0.84525351\n",
      "Iteration 34, loss = 0.84484526\n",
      "Iteration 35, loss = 0.84433311\n",
      "Iteration 36, loss = 0.84332140\n",
      "Iteration 37, loss = 0.84316676\n",
      "Iteration 38, loss = 0.84277182\n",
      "Iteration 39, loss = 0.84260242\n",
      "Iteration 40, loss = 0.84230102\n",
      "Iteration 41, loss = 0.84199766\n",
      "Iteration 42, loss = 0.84146477\n",
      "Iteration 43, loss = 0.84154030\n",
      "Iteration 44, loss = 0.84137328\n",
      "Iteration 45, loss = 0.84094078\n",
      "Iteration 46, loss = 0.84086302\n",
      "Iteration 47, loss = 0.84055277\n",
      "Iteration 48, loss = 0.84017868\n",
      "Iteration 49, loss = 0.83954879\n",
      "Iteration 50, loss = 0.83957706\n",
      "Iteration 51, loss = 0.83965237\n",
      "Iteration 52, loss = 0.83919971\n",
      "Iteration 53, loss = 0.83904340\n",
      "Iteration 54, loss = 0.83870351\n",
      "Iteration 55, loss = 0.83812307\n",
      "Iteration 56, loss = 0.83855801\n",
      "Iteration 57, loss = 0.83796184\n",
      "Iteration 58, loss = 0.83791744\n",
      "Iteration 59, loss = 0.83773542\n",
      "Iteration 60, loss = 0.83775501\n",
      "Iteration 61, loss = 0.83743220\n",
      "Iteration 62, loss = 0.83668231\n",
      "Iteration 63, loss = 0.83718607\n",
      "Iteration 64, loss = 0.83661638\n",
      "Iteration 65, loss = 0.83621333\n",
      "Iteration 66, loss = 0.83653538\n",
      "Iteration 67, loss = 0.83570386\n",
      "Iteration 68, loss = 0.83603163\n",
      "Iteration 69, loss = 0.83579150\n",
      "Iteration 70, loss = 0.83576412\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "clf10 score: 0.7031342826795326\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf10.fit(X_train, y_train)\n",
    "print('clf10 score: {}'.format(clf10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:25:25.712905Z",
     "start_time": "2018-05-19T21:25:23.826723Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 3513.20, NNZs: 25, Bias: -185.000000, T: 222186, Avg. loss: 432.477137\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2305.83, NNZs: 25, Bias: -443.000000, T: 222186, Avg. loss: 284.314215\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2415.66, NNZs: 25, Bias: -900.000000, T: 222186, Avg. loss: 952.459335\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3455.93, NNZs: 24, Bias: -796.000000, T: 222186, Avg. loss: 1618.235316Norm: 4650.69, NNZs: 25, Bias: 1155.000000, T: 222186, Avg. loss: 2515.775269\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5124.20, NNZs: 25, Bias: -440.000000, T: 444372, Avg. loss: 407.567589\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3098.77, NNZs: 25, Bias: -956.000000, T: 444372, Avg. loss: 277.028329\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3474.24, NNZs: 25, Bias: -1535.000000, T: 444372, Avg. loss: 945.686050\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4916.35, NNZs: 24, Bias: -1174.000000, T: 444372, Avg. loss: 1593.417829Norm: 6654.05, NNZs: 25, Bias: 1586.000000, T: 444372, Avg. loss: 2442.464925\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 3\n",
      "\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4172.13, NNZs: 25, Bias: -1993.000000, T: 666558, Avg. loss: 948.009248\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6190.31, NNZs: 25, Bias: -630.000000, T: 666558, Avg. loss: 396.774634Norm: 3513.00, NNZs: 25, Bias: -1389.000000, T: 666558, Avg. loss: 275.024909\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 4\n",
      "\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8099.06, NNZs: 25, Bias: 1760.000000, T: 666558, Avg. loss: 2435.167621\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4671.00, NNZs: 25, Bias: -2323.000000, T: 888744, Avg. loss: 946.939930Norm: 7120.55, NNZs: 25, Bias: -820.000000, T: 888744, Avg. loss: 389.881526Norm: 3829.52, NNZs: 25, Bias: -1756.000000, T: 888744, Avg. loss: 274.580670Norm: 5915.12, NNZs: 25, Bias: -1446.000000, T: 666558, Avg. loss: 1589.585130\n",
      "Total training time: 0.58 seconds.\n",
      "\n",
      "Total training time: 0.57 seconds.-- Epoch 4\n",
      "\n",
      "\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 5\n",
      "-- Epoch 5\n",
      "Total training time: 0.60 seconds.\n",
      "\n",
      "-- Epoch 5\n",
      "Norm: 9094.83, NNZs: 25, Bias: 1868.000000, T: 888744, Avg. loss: 2419.127571\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5098.38, NNZs: 25, Bias: -2597.000000, T: 1110930, Avg. loss: 951.525775Norm: 7762.63, NNZs: 25, Bias: -960.000000, T: 1110930, Avg. loss: 389.015743\n",
      "Total training time: 0.75 seconds.\n",
      "Norm: 9960.57, NNZs: 25, Bias: 1963.000000, T: 1110930, Avg. loss: 2418.457282\n",
      "Total training time: 0.75 seconds.\n",
      "\n",
      "Total training time: 0.80 seconds.\n",
      "Norm: 4127.61, NNZs: 25, Bias: -2093.000000, T: 1110930, Avg. loss: 275.381245\n",
      "Total training time: 0.79 seconds.\n",
      "Norm: 6663.79, NNZs: 25, Bias: -1635.000000, T: 888744, Avg. loss: 1588.811320\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.8s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7089.86, NNZs: 25, Bias: -1775.000000, T: 1110930, Avg. loss: 1593.633720\n",
      "Total training time: 0.96 seconds.\n",
      "clf12 score: 0.17268259311934037\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf12.fit(X_train, y_train)\n",
    "print('clf12 score: {}'.format(clf12.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:25:27.666733Z",
     "start_time": "2018-05-19T21:25:25.724286Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "Norm: 1.70, NNZs: 25, Bias: -0.771294, T: 222186, Avg. loss: 0.248748\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.22, NNZs: 25, Bias: -0.821673, T: 222186, Avg. loss: 0.585120\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.40, NNZs: 25, Bias: -0.455462, T: 222186, Avg. loss: 0.944740\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.75, NNZs: 25, Bias: -1.056379, T: 222186, Avg. loss: 0.171894Norm: 2.10, NNZs: 25, Bias: 0.298236, T: 222186, Avg. loss: 1.413759\n",
      "Total training time: 0.20 seconds.\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "\n",
      "Norm: 1.34, NNZs: 25, Bias: -0.842666, T: 444372, Avg. loss: 0.582708\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.83, NNZs: 25, Bias: -0.842348, T: 444372, Avg. loss: 0.235283\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.93, NNZs: 25, Bias: -1.216223, T: 444372, Avg. loss: 0.166643\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.32, NNZs: 25, Bias: 0.432157, T: 444372, Avg. loss: 1.394654\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.55, NNZs: 25, Bias: -0.616880, T: 444372, Avg. loss: 0.938944\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.45, NNZs: 25, Bias: -0.887259, T: 666558, Avg. loss: 0.583847\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.88, NNZs: 25, Bias: -0.818832, T: 666558, Avg. loss: 0.234349\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.63, NNZs: 25, Bias: -0.559854, T: 666558, Avg. loss: 0.940023Norm: 2.46, NNZs: 25, Bias: 0.414581, T: 666558, Avg. loss: 1.391597\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.98, NNZs: 25, Bias: -1.284872, T: 666558, Avg. loss: 0.164951\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 4\n",
      "\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.49, NNZs: 25, Bias: -0.864931, T: 888744, Avg. loss: 0.584055\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.39, NNZs: 25, Bias: 0.414103, T: 888744, Avg. loss: 1.393966Norm: 1.65, NNZs: 25, Bias: -0.529480, T: 888744, Avg. loss: 0.938311\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 5\n",
      "\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.90, NNZs: 25, Bias: -0.875170, T: 888744, Avg. loss: 0.233171\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.87, NNZs: 25, Bias: -1.312135, T: 888744, Avg. loss: 0.165653\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.52, NNZs: 25, Bias: -0.787500, T: 1110930, Avg. loss: 0.584484\n",
      "Total training time: 0.75 seconds.\n",
      "Norm: 1.65, NNZs: 25, Bias: -0.538995, T: 1110930, Avg. loss: 0.938974\n",
      "Total training time: 0.78 seconds.\n",
      "Norm: 1.95, NNZs: 25, Bias: -0.828221, T: 1110930, Avg. loss: 0.235883\n",
      "Total training time: 0.79 seconds.\n",
      "Norm: 2.55, NNZs: 25, Bias: 0.386673, T: 1110930, Avg. loss: 1.394430\n",
      "Total training time: 0.79 seconds.\n",
      "Norm: 0.97, NNZs: 25, Bias: -1.296593, T: 1110930, Avg. loss: 0.165391\n",
      "Total training time: 0.86 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.8s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf13 score: 0.32486002844438044\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf13.fit(X_train, y_train)\n",
    "print('clf13 score: {}'.format(clf13.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
