{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2\n",
    "\n",
    "- has **`location_raw`**: True\n",
    "- vars one-hot encoded: True\n",
    "- var label-encoded: False\n",
    "- oversampled: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T22:09:42.034014Z",
     "start_time": "2018-05-19T22:09:42.023149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import humanize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T22:06:45.332539Z",
     "start_time": "2018-05-19T22:06:44.476400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts:\n",
      "\ttrain: 222186\n",
      "\ttest: 55547\n"
     ]
    }
   ],
   "source": [
    "X_train = pickle.load(open('./data/stage2-train.pkl', 'rb'))\n",
    "y_train = X_train.pop('stop_outcome')\n",
    "X_test = pickle.load(open('./data/stage2-test.pkl', 'rb'))\n",
    "y_test = X_test.pop('stop_outcome')\n",
    "\n",
    "print('Row counts:\\n\\ttrain: {}\\n\\ttest: {}'.format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T13:11:49.870406Z",
     "start_time": "2018-05-19T13:11:49.868807Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features = pickle.load(open('./data/stage2-oh_encoded.pkl', 'rb'))\n",
    "# labels = features.pop('stop_outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T13:11:49.874223Z",
     "start_time": "2018-05-19T13:11:49.871221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the 'features' and 'labels' data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "#                                                     labels, \n",
    "#                                                     test_size=0.2, \n",
    "#                                                     random_state=0)\n",
    "# final_test_features = pickle.load(open('./data/final_test_set.pkl', 'rb'))\n",
    "# final_test_outcomes = final_test_features.pop('stop_outcome')\n",
    "# print('Final test set row count: {}'.format(final_test_features.shape[0]))\n",
    "\n",
    "# X_train = features\n",
    "# y_train = labels\n",
    "# X_test = final_test_features\n",
    "# y_test = final_test_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T13:11:50.977005Z",
     "start_time": "2018-05-19T13:11:49.875043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68210704448485071"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf_sgd = linear_model.SGDClassifier()\n",
    "clf_sgd.fit(X_train, y_train)\n",
    "clf_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T13:13:34.916932Z",
     "start_time": "2018-05-19T13:11:50.977930Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 10\n",
      "building tree 4 of 10building tree 1 of 10building tree 7 of 10building tree 2 of 10building tree 6 of 10building tree 8 of 10building tree 5 of 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of  10 | elapsed:    0.6s remaining:    1.3s\n",
      "[Parallel(n_jobs=8)]: Done   7 out of  10 | elapsed:    0.7s remaining:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      281155.7293            1.56m\n",
      "         2      266048.4388            1.54m\n",
      "         3      254042.7355            1.50m\n",
      "         4      244288.8940            1.48m\n",
      "         5      236316.8425            1.46m\n",
      "         6      229734.5819            1.45m\n",
      "         7      224223.7344            1.45m\n",
      "         8      219641.5698            1.44m\n",
      "         9      215744.3363            1.44m\n",
      "        10      212425.1033            1.43m\n",
      "        11      209634.3275            1.42m\n",
      "        12      207179.6033            1.41m\n",
      "        13      205104.2469            1.39m\n",
      "        14      203253.4101            1.38m\n",
      "        15      201609.5064            1.37m\n",
      "        16      200185.8394            1.36m\n",
      "        17      198971.0307            1.35m\n",
      "        18      197894.2861            1.34m\n",
      "        19      196914.3399            1.32m\n",
      "        20      196078.0901            1.31m\n",
      "        21      195294.3866            1.30m\n",
      "        22      194581.2043            1.29m\n",
      "        23      193958.3933            1.27m\n",
      "        24      193384.8295            1.26m\n",
      "        25      192785.7521            1.24m\n",
      "        26      192252.4338            1.22m\n",
      "        27      191788.3803            1.21m\n",
      "        28      191356.5719            1.20m\n",
      "        29      190942.0894            1.19m\n",
      "        30      190624.2732            1.17m\n",
      "        31      190265.9642            1.15m\n",
      "        32      189945.4551            1.14m\n",
      "        33      189682.4029            1.12m\n",
      "        34      189409.3886            1.11m\n",
      "        35      189142.8929            1.09m\n",
      "        36      188906.5128            1.07m\n",
      "        37      188719.9339            1.06m\n",
      "        38      188479.6572            1.04m\n",
      "        39      188279.7609            1.03m\n",
      "        40      188137.5832            1.01m\n",
      "        41      187936.0419           59.49s\n",
      "        42      187767.1386           58.41s\n",
      "        43      187563.3312           57.74s\n",
      "        44      187440.2702           56.59s\n",
      "        45      187183.0506           55.58s\n",
      "        46      186957.6885           54.59s\n",
      "        47      186839.6682           53.55s\n",
      "        48      186648.4490           52.52s\n",
      "        49      186512.1285           51.42s\n",
      "        50      186364.6190           50.33s\n",
      "        51      186263.2039           49.40s\n",
      "        52      186128.8152           48.38s\n",
      "        53      186038.9202           47.35s\n",
      "        54      185959.9988           46.26s\n",
      "        55      185820.1136           45.24s\n",
      "        56      185579.8063           44.28s\n",
      "        57      185471.5631           43.23s\n",
      "        58      185387.9605           42.29s\n",
      "        59      185304.4926           41.32s\n",
      "        60      185217.0165           40.30s\n",
      "        61      185129.7868           39.31s\n",
      "        62      184937.6620           38.30s\n",
      "        63      184836.8801           37.27s\n",
      "        64      184753.7605           36.21s\n",
      "        65      184658.8027           35.17s\n",
      "        66      184587.6609           34.16s\n",
      "        67      184486.9830           33.17s\n",
      "        68      184415.4156           32.13s\n",
      "        69      184355.0363           31.16s\n",
      "        70      184216.3762           30.15s\n",
      "        71      184130.0634           29.16s\n",
      "        72      183898.8673           28.18s\n",
      "        73      183836.1359           27.21s\n",
      "        74      183693.8090           26.22s\n",
      "        75      183616.0120           25.24s\n",
      "        76      183480.8398           24.26s\n",
      "        77      183420.4339           23.25s\n",
      "        78      183331.4218           22.24s\n",
      "        79      183288.4149           21.20s\n",
      "        80      183211.2129           20.18s\n",
      "        81      183139.6319           19.16s\n",
      "        82      183096.0283           18.16s\n",
      "        83      183030.1812           17.14s\n",
      "        84      182989.0597           16.12s\n",
      "        85      182893.9982           15.11s\n",
      "        86      182828.5258           14.13s\n",
      "        87      182760.5796           13.12s\n",
      "        88      182692.3098           12.12s\n",
      "        89      182610.9647           11.10s\n",
      "        90      182561.7549           10.08s\n",
      "        91      182488.8197            9.07s\n",
      "        92      182428.2592            8.06s\n",
      "        93      182395.2287            7.04s\n",
      "        94      182343.2792            6.03s\n",
      "        95      182308.0754            5.03s\n",
      "        96      182248.2857            4.02s\n",
      "        97      182140.2706            3.02s\n",
      "        98      182096.0204            2.01s\n",
      "        99      182052.2254            1.01s\n",
      "       100      182019.6523            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclf score: 0.65782130448089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf1 = RandomForestClassifier(n_jobs=8, verbose=3, random_state=0)\n",
    "clf2 = GaussianNB()\n",
    "clf3 = DecisionTreeClassifier(random_state=0)\n",
    "clf4 = GradientBoostingClassifier(verbose=3, random_state=0)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('rf', clf1), ('gnb', clf2), ('dt', clf3), ('gb', clf4)],\n",
    "                        voting='soft')\n",
    "eclf = eclf.fit(X_train, y_train)\n",
    "print('eclf score: {}'.format(eclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T13:13:34.920760Z",
     "start_time": "2018-05-19T13:13:34.917777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=8,\n",
       "             oob_score=False, random_state=0, verbose=3, warm_start=False),\n",
       " GaussianNB(priors=None),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "             splitter='best'),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               presort='auto', random_state=0, subsample=1.0, verbose=3,\n",
       "               warm_start=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclf.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T22:11:36.217132Z",
     "start_time": "2018-05-19T22:09:51.249680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7095432696635282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=0)\n",
    "with parallel_backend('threading'):\n",
    "    gbc.fit(X_train, y_train)\n",
    "print('{}'.format(gbc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingClassifier (Tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T22:17:34.488700Z",
     "start_time": "2018-05-19T22:11:36.218143Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1      251946.7001        2052.8657            5.50m\n",
      "         2      237414.4177        1575.6063            5.49m\n",
      "         3      226146.2405        1280.7294            5.44m\n",
      "         4      216676.8115        1016.1126            5.38m\n",
      "         5      209071.7951         835.2777            5.31m\n",
      "         6      202845.3294         693.5397            5.27m\n",
      "         7      197641.6092         569.6656            5.20m\n",
      "         8      193158.3705         474.1783            5.19m\n",
      "         9      189489.3661         400.0201            5.14m\n",
      "        10      186065.2030         343.1777            5.10m\n",
      "        11      183306.9784         299.3411            5.07m\n",
      "        12      180943.3302         233.9014            5.02m\n",
      "        13      179019.0402         232.2621            5.00m\n",
      "        14      177193.4287         193.0601            4.97m\n",
      "        15      175452.1221         159.1138            4.92m\n",
      "        16      173950.2571         142.2747            4.88m\n",
      "        17      172701.8591         147.2604            4.84m\n",
      "        18      171643.2669         111.5289            4.80m\n",
      "        19      170402.2973         110.0611            4.75m\n",
      "        20      169525.6706          93.5550            4.71m\n",
      "        21      168543.6776          85.4157            4.65m\n",
      "        22      167897.0430          70.4670            4.60m\n",
      "        23      167201.4881          67.9853            4.54m\n",
      "        24      166411.1871          65.5720            4.49m\n",
      "        25      166005.0180          48.4828            4.44m\n",
      "        26      165215.1842          62.8770            4.39m\n",
      "        27      164739.6685          38.9297            4.33m\n",
      "        28      164459.3482          34.0634            4.27m\n",
      "        29      163911.8259          36.3086            4.21m\n",
      "        30      163424.0217          42.9450            4.15m\n",
      "        31      163025.6433          23.7997            4.08m\n",
      "        32      162781.6794          22.8516            4.03m\n",
      "        33      162534.1818          31.1526            3.98m\n",
      "        34      161801.4370          38.1011            3.94m\n",
      "        35      161805.8159          15.2977            3.88m\n",
      "        36      161629.6114          19.8749            3.81m\n",
      "        37      161324.9509          15.8554            3.76m\n",
      "        38      160956.7897          24.4457            3.70m\n",
      "        39      160686.9688          17.1090            3.65m\n",
      "        40      160385.3908          18.9075            3.59m\n",
      "        41      160422.7605          12.2415            3.53m\n",
      "        42      159838.7964          14.9555            3.47m\n",
      "        43      159681.6892          20.6836            3.42m\n",
      "        44      159603.5642           5.7448            3.37m\n",
      "        45      159359.3345          11.5748            3.31m\n",
      "        46      159146.6396           9.8964            3.24m\n",
      "        47      158992.3302          14.5476            3.18m\n",
      "        48      158880.0760          10.4381            3.12m\n",
      "        49      158591.1340           2.7940            3.06m\n",
      "        50      158291.4322           9.9477            3.00m\n",
      "        51      158530.8297           7.2470            2.94m\n",
      "        52      157910.2758           5.9036            2.88m\n",
      "        53      157776.9739           7.8753            2.82m\n",
      "        54      157697.7992           5.9987            2.76m\n",
      "        55      157779.7929           3.5206            2.70m\n",
      "        56      157463.8003           6.3886            2.63m\n",
      "        57      157430.9715           4.3033            2.57m\n",
      "        58      157193.2913           5.5739            2.51m\n",
      "        59      157014.4025           7.3814            2.45m\n",
      "        60      156965.7790           3.1290            2.39m\n",
      "        61      156728.0926           3.2582            2.33m\n",
      "        62      156705.9051           4.9112            2.27m\n",
      "        63      156676.3573           2.6202            2.22m\n",
      "        64      156581.3084           3.2252            2.16m\n",
      "        65      156646.1104           5.2890            2.09m\n",
      "        66      156161.2550           1.9690            2.03m\n",
      "        67      156146.1552           0.2324            1.97m\n",
      "        68      156090.1757           0.2368            1.91m\n",
      "        69      155763.6007           9.4142            1.85m\n",
      "        70      155883.4752           9.2075            1.79m\n",
      "        71      155556.1040           3.2030            1.74m\n",
      "        72      155517.8928           1.7893            1.67m\n",
      "        73      155197.8722           0.5613            1.61m\n",
      "        74      155433.3657           2.8311            1.55m\n",
      "        75      155199.0529           5.2212            1.49m\n",
      "        76      155161.4172           2.0472            1.43m\n",
      "        77      154763.3395           1.2017            1.37m\n",
      "        78      154776.3181           1.4091            1.32m\n",
      "        79      154866.5707           2.2818            1.26m\n",
      "        80      154920.6218           0.4825            1.20m\n",
      "        81      154642.2299           3.0284            1.14m\n",
      "        82      154508.0181          -0.0099            1.08m\n",
      "        83      154409.4344           2.8622            1.02m\n",
      "        84      154362.0253           2.9633           57.39s\n",
      "        85      154097.0346           0.9676           53.77s\n",
      "        86      154099.1491           1.3204           50.19s\n",
      "        87      153878.1257           0.2577           46.59s\n",
      "        88      153955.7513          -0.3924           42.99s\n",
      "        89      153807.9836          -0.0847           39.36s\n",
      "        90      153859.1388           1.2304           35.74s\n",
      "        91      153665.2263           2.4978           32.18s\n",
      "        92      153564.8295           1.3433           28.61s\n",
      "        93      153479.7289          -0.5297           25.02s\n",
      "        94      153298.9618           0.7541           21.42s\n",
      "        95      153260.4832           1.1720           17.85s\n",
      "        96      153127.5509           0.6926           14.28s\n",
      "        97      153182.5734           0.8543           10.71s\n",
      "        98      152892.1275           0.8075            7.14s\n",
      "        99      152875.3652           0.0160            3.57s\n",
      "       100      152839.0991          -1.8317            0.00s\n",
      "0.716618359227321\n"
     ]
    }
   ],
   "source": [
    "gbc_tuned = GradientBoostingClassifier(\n",
    "    learning_rate=0.0983,\n",
    "    max_depth=6,\n",
    "    max_features=len(list(X_train.columns.values)),\n",
    "    subsample=0.9,\n",
    "    verbose=3,\n",
    "    random_state=0,\n",
    ")\n",
    "with parallel_backend('threading'):\n",
    "    gbc_tuned.fit(X_train, y_train)\n",
    "print('{}'.format(gbc_tuned.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
