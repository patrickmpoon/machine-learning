{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:24:57.390470Z",
     "start_time": "2018-05-21T00:24:56.580348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import humanize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:24:57.828028Z",
     "start_time": "2018-05-21T00:24:57.391173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts:\n",
      "\ttrain: 250247\n",
      "\ttest: 62562\n"
     ]
    }
   ],
   "source": [
    "X_train = pickle.load(open('./data/stage6-train.pkl', 'rb'))\n",
    "y_train = X_train.pop('stop_outcome')\n",
    "X_test = pickle.load(open('./data/stage6-test.pkl', 'rb'))\n",
    "y_test = X_test.pop('stop_outcome')\n",
    "\n",
    "print('Row counts:\\n\\ttrain: {}\\n\\ttest: {}'.format(X_train.shape[0], X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:24:57.896772Z",
     "start_time": "2018-05-21T00:24:57.828842Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, PassiveAggressiveClassifier, Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier as EnsExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "# clf1 = RandomForestClassifier(n_jobs=8, verbose=3, random_state=0)\n",
    "# clf2 = GaussianNB()\n",
    "# clf3 = DecisionTreeClassifier(random_state=0)\n",
    "# clf4 = GradientBoostingClassifier(verbose=3, random_state=0)\n",
    "\n",
    "# eclf = VotingClassifier(estimators=[('rf', clf1), ('gnb', clf2), ('dt', clf3), ('gb', clf4)],\n",
    "#                         voting='soft')\n",
    "\n",
    "clf5 = BernoulliNB()\n",
    "clf6 = ExtraTreeClassifier()\n",
    "clf7 = EnsExtraTreesClassifier(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf8 = KNeighborsClassifier(n_jobs=-1)\n",
    "clf9 = LogisticRegressionCV(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf10 = MLPClassifier(verbose=3, random_state=0)\n",
    "# clf11 = LinearSVC(verbose=3, random_state=0)\n",
    "clf12 = Perceptron(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf13 = PassiveAggressiveClassifier(n_jobs=-1, verbose=3, random_state=0)\n",
    "# clf14 = ()\n",
    "# clf15 = ()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[\n",
    "                            ('bnb', clf5),\n",
    "                            ('etc', clf6),\n",
    "                            ('eetc', clf7),\n",
    "                            ('knc', clf8),\n",
    "                            ('lrcv', clf9),\n",
    "                            ('mlpc', clf10),\n",
    "#                             ('lsvc', clf11),\n",
    "                            ('p', clf12),\n",
    "                            ('pac', clf13),\n",
    "#                             ('', clf14),\n",
    "#                             ('', clf15),\n",
    "                        ], voting='soft')\n",
    "\n",
    "# with parallel_backend('threading'):\n",
    "#     eclf = eclf.fit(X_train, y_train)\n",
    "# print('eclf score: {}'.format(eclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:24:58.039915Z",
     "start_time": "2018-05-21T00:24:57.897562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf5 score: 0.7062114382532527\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf5.fit(X_train, y_train)\n",
    "print('clf5 score: {}'.format(clf5.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:24:58.301541Z",
     "start_time": "2018-05-21T00:24:58.040646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf6 score: 0.5775071129439596\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf6.fit(X_train, y_train)\n",
    "print('clf6 score: {}'.format(clf6.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:24:59.256914Z",
     "start_time": "2018-05-21T00:24:58.302413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 10building tree 1 of 10building tree 3 of 10building tree 4 of 10building tree 6 of 10building tree 5 of 10building tree 7 of 10building tree 8 of 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    0.5s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    0.6s remaining:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf7 score: 0.7006969086666027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=8)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf7.fit(X_train, y_train)\n",
    "print('clf7 score: {}'.format(clf7.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:25:00.731485Z",
     "start_time": "2018-05-21T00:24:59.258420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf8 score: 0.6596336434257217\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf8.fit(X_train, y_train)\n",
    "print('clf8 score: {}'.format(clf8.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:28:41.865967Z",
     "start_time": "2018-05-21T00:25:00.733099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:  2.1min remaining:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  3.4min remaining:   50.4s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  3.5min finished\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf9 score: 0.6999136856238611\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf9.fit(X_train, y_train)\n",
    "print('clf9 score: {}'.format(clf9.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:30:27.209274Z",
     "start_time": "2018-05-21T00:28:41.881998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.08283376\n",
      "Iteration 2, loss = 0.84213018\n",
      "Iteration 3, loss = 0.83716190\n",
      "Iteration 4, loss = 0.83439870\n",
      "Iteration 5, loss = 0.83256846\n",
      "Iteration 6, loss = 0.82972461\n",
      "Iteration 7, loss = 0.82904573\n",
      "Iteration 8, loss = 0.82612269\n",
      "Iteration 9, loss = 0.82599947\n",
      "Iteration 10, loss = 0.82526479\n",
      "Iteration 11, loss = 0.82427751\n",
      "Iteration 12, loss = 0.82123958\n",
      "Iteration 13, loss = 0.81989773\n",
      "Iteration 14, loss = 0.81836311\n",
      "Iteration 15, loss = 0.81621964\n",
      "Iteration 16, loss = 0.81590480\n",
      "Iteration 17, loss = 0.81376682\n",
      "Iteration 18, loss = 0.81301322\n",
      "Iteration 19, loss = 0.81216099\n",
      "Iteration 20, loss = 0.81210284\n",
      "Iteration 21, loss = 0.81047832\n",
      "Iteration 22, loss = 0.80806425\n",
      "Iteration 23, loss = 0.80620564\n",
      "Iteration 24, loss = 0.80470544\n",
      "Iteration 25, loss = 0.80359967\n",
      "Iteration 26, loss = 0.80257615\n",
      "Iteration 27, loss = 0.80156174\n",
      "Iteration 28, loss = 0.80082386\n",
      "Iteration 29, loss = 0.80026544\n",
      "Iteration 30, loss = 0.79974956\n",
      "Iteration 31, loss = 0.79915747\n",
      "Iteration 32, loss = 0.79894742\n",
      "Iteration 33, loss = 0.79827758\n",
      "Iteration 34, loss = 0.79790323\n",
      "Iteration 35, loss = 0.79766875\n",
      "Iteration 36, loss = 0.79694873\n",
      "Iteration 37, loss = 0.79633365\n",
      "Iteration 38, loss = 0.79637514\n",
      "Iteration 39, loss = 0.79601350\n",
      "Iteration 40, loss = 0.79622387\n",
      "Iteration 41, loss = 0.79584631\n",
      "Iteration 42, loss = 0.79526942\n",
      "Iteration 43, loss = 0.79513086\n",
      "Iteration 44, loss = 0.79463560\n",
      "Iteration 45, loss = 0.79427777\n",
      "Iteration 46, loss = 0.79373232\n",
      "Iteration 47, loss = 0.79365529\n",
      "Iteration 48, loss = 0.79377930\n",
      "Iteration 49, loss = 0.79332678\n",
      "Iteration 50, loss = 0.79286617\n",
      "Iteration 51, loss = 0.79283309\n",
      "Iteration 52, loss = 0.79236943\n",
      "Iteration 53, loss = 0.79188409\n",
      "Iteration 54, loss = 0.79184860\n",
      "Iteration 55, loss = 0.79188136\n",
      "Iteration 56, loss = 0.79122113\n",
      "Iteration 57, loss = 0.79090048\n",
      "Iteration 58, loss = 0.79043284\n",
      "Iteration 59, loss = 0.79014271\n",
      "Iteration 60, loss = 0.78984941\n",
      "Iteration 61, loss = 0.78955560\n",
      "Iteration 62, loss = 0.78918390\n",
      "Iteration 63, loss = 0.78914896\n",
      "Iteration 64, loss = 0.78850483\n",
      "Iteration 65, loss = 0.78829380\n",
      "Iteration 66, loss = 0.78771823\n",
      "Iteration 67, loss = 0.78783657\n",
      "Iteration 68, loss = 0.78779664\n",
      "Iteration 69, loss = 0.78666947\n",
      "Iteration 70, loss = 0.78654639\n",
      "Iteration 71, loss = 0.78668248\n",
      "Iteration 72, loss = 0.78664156\n",
      "Iteration 73, loss = 0.78678333\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "clf10 score: 0.7149387807295163\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf10.fit(X_train, y_train)\n",
    "print('clf10 score: {}'.format(clf10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:30:27.589082Z",
     "start_time": "2018-05-21T00:30:27.213030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4156.34, NNZs: 24, Bias: -282.000000, T: 250247, Avg. loss: 439.934588\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5808.71, NNZs: 24, Bias: -29.000000, T: 250247, Avg. loss: 2740.170827Norm: 2700.61, NNZs: 24, Bias: -356.000000, T: 250247, Avg. loss: 1027.073419\n",
      "\n",
      "Total training time: 0.04 seconds.Total training time: 0.04 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n",
      "Norm: 2831.39, NNZs: 24, Bias: -203.000000, T: 250247, Avg. loss: 286.031067\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4286.04, NNZs: 24, Bias: 58.000000, T: 250247, Avg. loss: 1744.417352\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4124.35, NNZs: 24, Bias: -594.000000, T: 500494, Avg. loss: 1018.337623Norm: 5907.44, NNZs: 24, Bias: -632.000000, T: 500494, Avg. loss: 412.374045\n",
      "\n",
      "Total training time: 0.08 seconds.Total training time: 0.08 seconds.\n",
      "\n",
      "-- Epoch 3-- Epoch 3Norm: 8414.09, NNZs: 24, Bias: 39.000000, T: 500494, Avg. loss: 2660.616158\n",
      "\n",
      "\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3863.60, NNZs: 24, Bias: -534.000000, T: 500494, Avg. loss: 279.076095\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6244.94, NNZs: 24, Bias: 32.000000, T: 500494, Avg. loss: 1708.410669\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10096.31, NNZs: 24, Bias: 78.000000, T: 750741, Avg. loss: 2630.958702Norm: 5005.68, NNZs: 24, Bias: -814.000000, T: 750741, Avg. loss: 1015.915136\n",
      "\n",
      "Norm: 7123.17, NNZs: 24, Bias: -957.000000, T: 750741, Avg. loss: 405.891043Total training time: 0.11 seconds.Total training time: 0.11 seconds.\n",
      "\n",
      "\n",
      "Total training time: 0.12 seconds.-- Epoch 4-- Epoch 4\n",
      "\n",
      "\n",
      "-- Epoch 4\n",
      "Norm: 4355.49, NNZs: 24, Bias: -914.000000, T: 750741, Avg. loss: 281.357673\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7368.53, NNZs: 24, Bias: -101.000000, T: 750741, Avg. loss: 1696.053984\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5640.41, NNZs: 24, Bias: -1008.000000, T: 1000988, Avg. loss: 1021.081562\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11385.77, NNZs: 24, Bias: 174.000000, T: 1000988, Avg. loss: 2626.259359\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8003.54, NNZs: 24, Bias: -1181.000000, T: 1000988, Avg. loss: 398.559417\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8304.64, NNZs: 24, Bias: -237.000000, T: 1000988, Avg. loss: 1694.710512Norm: 4629.33, NNZs: 24, Bias: -1241.000000, T: 1000988, Avg. loss: 281.882641\n",
      "\n",
      "Total training time: 0.17 seconds.Total training time: 0.17 seconds.\n",
      "\n",
      "-- Epoch 5-- Epoch 5\n",
      "\n",
      "Norm: 6117.93, NNZs: 24, Bias: -1187.000000, T: 1251235, Avg. loss: 1016.633810\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 12447.03, NNZs: 24, Bias: 242.000000, T: 1251235, Avg. loss: 2618.181040Norm: 8812.31, NNZs: 24, Bias: -1433.000000, T: 1251235, Avg. loss: 397.277319\n",
      "Total training time: 0.20 seconds.\n",
      "\n",
      "Total training time: 0.20 seconds.\n",
      "Norm: 4885.73, NNZs: 24, Bias: -1541.000000, T: 1251235, Avg. loss: 282.747907\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 9006.86, NNZs: 24, Bias: -316.000000, T: 1251235, Avg. loss: 1695.342580\n",
      "Total training time: 0.21 seconds.\n",
      "clf12 score: 0.6438732777085131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf12.fit(X_train, y_train)\n",
    "print('clf12 score: {}'.format(clf12.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T00:30:27.958757Z",
     "start_time": "2018-05-21T00:30:27.589946Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 0.76, NNZs: 24, Bias: -0.485293, T: 250247, Avg. loss: 0.431053\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.77, NNZs: 24, Bias: -0.468572, T: 250247, Avg. loss: 0.130663\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.18, NNZs: 24, Bias: -0.487094, T: 250247, Avg. loss: 0.191602\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.52, NNZs: 24, Bias: 0.237861, T: 250247, Avg. loss: 1.063597\n",
      "Norm: 1.17, NNZs: 24, Bias: -0.351349, T: 250247, Avg. loss: 0.712471\n",
      "Total training time: 0.05 seconds.Total training time: 0.05 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n",
      "Norm: 1.51, NNZs: 24, Bias: -0.659217, T: 500494, Avg. loss: 0.174573\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.97, NNZs: 24, Bias: -0.671115, T: 500494, Avg. loss: 0.422989\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.88, NNZs: 24, Bias: -0.691385, T: 500494, Avg. loss: 0.126518\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.98, NNZs: 24, Bias: 0.276925, T: 500494, Avg. loss: 1.032564\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.51, NNZs: 24, Bias: -0.391739, T: 500494, Avg. loss: 0.695909\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.68, NNZs: 24, Bias: -0.754427, T: 750741, Avg. loss: 0.171187\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.94, NNZs: 24, Bias: -0.819652, T: 750741, Avg. loss: 0.125020\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.08, NNZs: 24, Bias: -0.718513, T: 750741, Avg. loss: 0.424947\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.21, NNZs: 24, Bias: 0.297383, T: 750741, Avg. loss: 1.024108Norm: 1.67, NNZs: 24, Bias: -0.453229, T: 750741, Avg. loss: 0.693466\n",
      "Total training time: 0.16 seconds.\n",
      "\n",
      "Total training time: 0.16 seconds.-- Epoch 4\n",
      "\n",
      "-- Epoch 4\n",
      "Norm: 1.80, NNZs: 24, Bias: -0.803930, T: 1000988, Avg. loss: 0.169622\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.95, NNZs: 24, Bias: -0.919580, T: 1000988, Avg. loss: 0.126165\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.16, NNZs: 24, Bias: -0.777638, T: 1000988, Avg. loss: 0.426805\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.87, NNZs: 24, Bias: -0.836035, T: 1251235, Avg. loss: 0.170311\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 1.00, NNZs: 24, Bias: -0.987895, T: 1251235, Avg. loss: 0.125102\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 2.35, NNZs: 24, Bias: 0.348269, T: 1000988, Avg. loss: 1.024097Norm: 1.80, NNZs: 24, Bias: -0.486600, T: 1000988, Avg. loss: 0.694715\n",
      "Total training time: 0.21 seconds.\n",
      "\n",
      "Total training time: 0.21 seconds.-- Epoch 5\n",
      "\n",
      "-- Epoch 5\n",
      "Norm: 1.21, NNZs: 24, Bias: -0.816618, T: 1251235, Avg. loss: 0.425051\n",
      "Total training time: 0.22 seconds.\n",
      "Norm: 1.86, NNZs: 24, Bias: -0.501699, T: 1251235, Avg. loss: 0.692634Norm: 2.44, NNZs: 24, Bias: 0.361245, T: 1251235, Avg. loss: 1.021053\n",
      "\n",
      "Total training time: 0.26 seconds.Total training time: 0.26 seconds.\n",
      "\n",
      "clf13 score: 0.7011124964035677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf13.fit(X_train, y_train)\n",
    "print('clf13 score: {}'.format(clf13.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
