{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2\n",
    "\n",
    "- has **`location_raw`**: True\n",
    "- vars one-hot encoded: True\n",
    "- var label-encoded: False\n",
    "- oversampled: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T20:41:53.408350Z",
     "start_time": "2018-05-19T20:41:49.901306Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import humanize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T20:41:54.467095Z",
     "start_time": "2018-05-19T20:41:53.427326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts:\n",
      "\ttrain: 222186\n",
      "\ttest: 55547\n"
     ]
    }
   ],
   "source": [
    "X_train = pickle.load(open('./data/stage2-train.pkl', 'rb'))\n",
    "y_train = X_train.pop('stop_outcome')\n",
    "X_test = pickle.load(open('./data/stage2-test.pkl', 'rb'))\n",
    "y_test = X_test.pop('stop_outcome')\n",
    "\n",
    "print('Row counts:\\n\\ttrain: {}\\n\\ttest: {}'.format(X_train.shape[0], X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T20:41:54.754622Z",
     "start_time": "2018-05-19T20:41:54.490283Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, PassiveAggressiveClassifier, Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier as EnsExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "# clf1 = RandomForestClassifier(n_jobs=8, verbose=3, random_state=0)\n",
    "# clf2 = GaussianNB()\n",
    "# clf3 = DecisionTreeClassifier(random_state=0)\n",
    "# clf4 = GradientBoostingClassifier(verbose=3, random_state=0)\n",
    "\n",
    "# eclf = VotingClassifier(estimators=[('rf', clf1), ('gnb', clf2), ('dt', clf3), ('gb', clf4)],\n",
    "#                         voting='soft')\n",
    "\n",
    "clf5 = BernoulliNB()\n",
    "clf6 = ExtraTreeClassifier()\n",
    "clf7 = EnsExtraTreesClassifier(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf8 = KNeighborsClassifier(n_jobs=-1)\n",
    "clf9 = LogisticRegressionCV(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf10 = MLPClassifier(verbose=3, random_state=0)\n",
    "# clf11 = LinearSVC(verbose=3, random_state=0)\n",
    "clf12 = Perceptron(n_jobs=-1, verbose=3, random_state=0)\n",
    "clf13 = PassiveAggressiveClassifier(n_jobs=-1, verbose=3, random_state=0)\n",
    "# clf14 = ()\n",
    "# clf15 = ()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[\n",
    "                            ('bnb', clf5),\n",
    "                            ('etc', clf6),\n",
    "                            ('eetc', clf7),\n",
    "                            ('knc', clf8),\n",
    "                            ('lrcv', clf9),\n",
    "                            ('mlpc', clf10),\n",
    "#                             ('lsvc', clf11),\n",
    "                            ('p', clf12),\n",
    "                            ('pac', clf13),\n",
    "#                             ('', clf14),\n",
    "#                             ('', clf15),\n",
    "                        ], voting='soft')\n",
    "\n",
    "# with parallel_backend('threading'):\n",
    "#     eclf = eclf.fit(X_train, y_train)\n",
    "# print('eclf score: {}'.format(eclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T20:41:58.129153Z",
     "start_time": "2018-05-19T20:41:54.778451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf5 score: 0.6760941184942482\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf5.fit(X_train, y_train)\n",
    "print('clf5 score: {}'.format(clf5.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T20:42:01.365172Z",
     "start_time": "2018-05-19T20:41:58.138438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf6 score: 0.5472482762345401\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf6.fit(X_train, y_train)\n",
    "print('clf6 score: {}'.format(clf6.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T20:42:08.845914Z",
     "start_time": "2018-05-19T20:42:01.372613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n",
      "building tree 2 of 10building tree 3 of 10building tree 4 of 10building tree 5 of 10building tree 6 of 10building tree 7 of 10building tree 8 of 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    3.1s remaining:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    3.3s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=8)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done   7 out of  10 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf7 score: 0.5906889661007795\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf7.fit(X_train, y_train)\n",
    "print('clf7 score: {}'.format(clf7.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T20:42:33.852642Z",
     "start_time": "2018-05-19T20:42:08.859903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf8 score: 0.6623580031324824\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf8.fit(X_train, y_train)\n",
    "print('clf8 score: {}'.format(clf8.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T20:47:46.153921Z",
     "start_time": "2018-05-19T20:42:33.860302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:  1.9min remaining:  2.8min\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  3.5min remaining:   52.2s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  3.8min finished\n",
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf9 score: 0.6895601922696095\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf9.fit(X_train, y_train)\n",
    "print('clf9 score: {}'.format(clf9.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:05:51.310433Z",
     "start_time": "2018-05-19T20:47:46.164608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.92447632\n",
      "Iteration 2, loss = 0.85916500\n",
      "Iteration 3, loss = 0.85710948\n",
      "Iteration 4, loss = 0.85427295\n",
      "Iteration 5, loss = 0.85039095\n",
      "Iteration 6, loss = 0.84743005\n",
      "Iteration 7, loss = 0.84659769\n",
      "Iteration 8, loss = 0.84357961\n",
      "Iteration 9, loss = 0.84072750\n",
      "Iteration 10, loss = 0.83971267\n",
      "Iteration 11, loss = 0.83933775\n",
      "Iteration 12, loss = 0.83608377\n",
      "Iteration 13, loss = 0.83775271\n",
      "Iteration 14, loss = 0.83493163\n",
      "Iteration 15, loss = 0.83277141\n",
      "Iteration 16, loss = 0.83178771\n",
      "Iteration 17, loss = 0.83129448\n",
      "Iteration 18, loss = 0.83198876\n",
      "Iteration 19, loss = 0.83014413\n",
      "Iteration 20, loss = 0.82853905\n",
      "Iteration 21, loss = 0.82775009\n",
      "Iteration 22, loss = 0.82699032\n",
      "Iteration 23, loss = 0.82673587\n",
      "Iteration 24, loss = 0.82548647\n",
      "Iteration 25, loss = 0.82539860\n",
      "Iteration 26, loss = 0.82453541\n",
      "Iteration 27, loss = 0.82370634\n",
      "Iteration 28, loss = 0.82306979\n",
      "Iteration 29, loss = 0.82279872\n",
      "Iteration 30, loss = 0.82232195\n",
      "Iteration 31, loss = 0.82142304\n",
      "Iteration 32, loss = 0.82112920\n",
      "Iteration 33, loss = 0.82038568\n",
      "Iteration 34, loss = 0.81983406\n",
      "Iteration 35, loss = 0.81971895\n",
      "Iteration 36, loss = 0.81875575\n",
      "Iteration 37, loss = 0.81874664\n",
      "Iteration 38, loss = 0.81824490\n",
      "Iteration 39, loss = 0.81780613\n",
      "Iteration 40, loss = 0.81748867\n",
      "Iteration 41, loss = 0.81664971\n",
      "Iteration 42, loss = 0.81672756\n",
      "Iteration 43, loss = 0.81639670\n",
      "Iteration 44, loss = 0.81633401\n",
      "Iteration 45, loss = 0.81593547\n",
      "Iteration 46, loss = 0.81616312\n",
      "Iteration 47, loss = 0.81520244\n",
      "Iteration 48, loss = 0.81506636\n",
      "Iteration 49, loss = 0.81469586\n",
      "Iteration 50, loss = 0.81517882\n",
      "Iteration 51, loss = 0.81446444\n",
      "Iteration 52, loss = 0.81388700\n",
      "Iteration 53, loss = 0.81405542\n",
      "Iteration 54, loss = 0.81442394\n",
      "Iteration 55, loss = 0.81375908\n",
      "Iteration 56, loss = 0.81356838\n",
      "Iteration 57, loss = 0.81305457\n",
      "Iteration 58, loss = 0.81292677\n",
      "Iteration 59, loss = 0.81317180\n",
      "Iteration 60, loss = 0.81307190\n",
      "Iteration 61, loss = 0.81235077\n",
      "Iteration 62, loss = 0.81264819\n",
      "Iteration 63, loss = 0.81216566\n",
      "Iteration 64, loss = 0.81186791\n",
      "Iteration 65, loss = 0.81181147\n",
      "Iteration 66, loss = 0.81166608\n",
      "Iteration 67, loss = 0.81167637\n",
      "Iteration 68, loss = 0.81137108\n",
      "Iteration 69, loss = 0.81076909\n",
      "Iteration 70, loss = 0.81105748\n",
      "Iteration 71, loss = 0.81079293\n",
      "Iteration 72, loss = 0.81094479\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "clf10 score: 0.7091112031252813\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf10.fit(X_train, y_train)\n",
    "print('clf10 score: {}'.format(clf10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:05:54.157116Z",
     "start_time": "2018-05-19T21:05:51.323799Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 3942.12, NNZs: 44, Bias: -58.000000, T: 222186, Avg. loss: 427.683598\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2487.34, NNZs: 44, Bias: -141.000000, T: 222186, Avg. loss: 283.862493\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3610.14, NNZs: 44, Bias: -623.000000, T: 222186, Avg. loss: 1607.267143Norm: 2983.06, NNZs: 44, Bias: -562.000000, T: 222186, Avg. loss: 941.293564Norm: 5170.58, NNZs: 44, Bias: -413.000000, T: 222186, Avg. loss: 2484.531709\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5780.01, NNZs: 44, Bias: -162.000000, T: 444372, Avg. loss: 391.531575\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3724.31, NNZs: 43, Bias: -293.000000, T: 444372, Avg. loss: 272.918833\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5148.24, NNZs: 44, Bias: -994.000000, T: 444372, Avg. loss: 1578.475784\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4275.92, NNZs: 44, Bias: -937.000000, T: 444372, Avg. loss: 935.149787\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7458.03, NNZs: 44, Bias: -586.000000, T: 444372, Avg. loss: 2394.795194Norm: 7106.93, NNZs: 44, Bias: -193.000000, T: 666558, Avg. loss: 383.289283\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 3\n",
      "\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4506.83, NNZs: 44, Bias: -358.000000, T: 666558, Avg. loss: 272.450539\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6201.59, NNZs: 44, Bias: -1264.000000, T: 666558, Avg. loss: 1570.203909\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5126.38, NNZs: 44, Bias: -1147.000000, T: 666558, Avg. loss: 931.293253\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8154.16, NNZs: 44, Bias: -217.000000, T: 888744, Avg. loss: 376.735566\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4998.75, NNZs: 44, Bias: -447.000000, T: 888744, Avg. loss: 271.184343\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8915.58, NNZs: 44, Bias: -692.000000, T: 666558, Avg. loss: 2383.709400\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6967.81, NNZs: 44, Bias: -1449.000000, T: 888744, Avg. loss: 1578.715698\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5671.84, NNZs: 44, Bias: -1284.000000, T: 888744, Avg. loss: 936.763670\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5387.65, NNZs: 44, Bias: -508.000000, T: 1110930, Avg. loss: 272.054822\n",
      "Total training time: 1.02 seconds.\n",
      "Norm: 8915.58, NNZs: 44, Bias: -301.000000, T: 1110930, Avg. loss: 377.452970\n",
      "Total training time: 1.13 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.1s remaining:    1.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7513.05, NNZs: 44, Bias: -1574.000000, T: 1110930, Avg. loss: 1587.280608\n",
      "Total training time: 1.17 seconds.\n",
      "Norm: 6039.99, NNZs: 44, Bias: -1428.000000, T: 1110930, Avg. loss: 940.814813\n",
      "Total training time: 1.20 seconds.\n",
      "Norm: 9997.66, NNZs: 44, Bias: -737.000000, T: 888744, Avg. loss: 2374.471112\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10856.99, NNZs: 44, Bias: -723.000000, T: 1110930, Avg. loss: 2368.009656\n",
      "Total training time: 1.45 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf12 score: 0.199290690766378\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf12.fit(X_train, y_train)\n",
    "print('clf12 score: {}'.format(clf12.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:05:56.946346Z",
     "start_time": "2018-05-19T21:05:54.169702Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pato/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 1.53, NNZs: 44, Bias: -0.311938, T: 222186, Avg. loss: 0.182589Norm: 2.43, NNZs: 44, Bias: -0.246414, T: 222186, Avg. loss: 0.265065\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.08, NNZs: 44, Bias: -0.494228, T: 222186, Avg. loss: 0.651753\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.96, NNZs: 44, Bias: -0.358020, T: 222186, Avg. loss: 1.582261Norm: 2.35, NNZs: 44, Bias: -0.351305, T: 222186, Avg. loss: 1.102906\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.43, NNZs: 44, Bias: -0.330333, T: 444372, Avg. loss: 0.263181\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.45, NNZs: 44, Bias: -0.435095, T: 444372, Avg. loss: 0.184653\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.07, NNZs: 44, Bias: -0.527605, T: 444372, Avg. loss: 0.655245\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.30, NNZs: 44, Bias: -0.694518, T: 444372, Avg. loss: 1.096662\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.47, NNZs: 44, Bias: -0.269058, T: 666558, Avg. loss: 0.184856Norm: 3.07, NNZs: 44, Bias: -0.043916, T: 444372, Avg. loss: 1.565298\n",
      "Total training time: 0.70 seconds.\n",
      "\n",
      "-- Epoch 4Total training time: 0.71 seconds.\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 2.52, NNZs: 44, Bias: -0.448666, T: 666558, Avg. loss: 0.260690\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.25, NNZs: 44, Bias: -0.655401, T: 666558, Avg. loss: 0.652647\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.76, NNZs: 44, Bias: -0.361767, T: 666558, Avg. loss: 1.099508Norm: 1.44, NNZs: 44, Bias: -0.379651, T: 888744, Avg. loss: 0.183092\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 5\n",
      "\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.36, NNZs: 44, Bias: -0.386654, T: 666558, Avg. loss: 1.560987\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.40, NNZs: 44, Bias: -0.400451, T: 888744, Avg. loss: 0.261116\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.28, NNZs: 44, Bias: 0.021952, T: 888744, Avg. loss: 1.570381\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.48, NNZs: 44, Bias: -0.527610, T: 888744, Avg. loss: 1.096494\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.56, NNZs: 44, Bias: -0.332190, T: 1110930, Avg. loss: 0.183891Norm: 2.41, NNZs: 44, Bias: -0.712608, T: 888744, Avg. loss: 0.658193\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 5\n",
      "\n",
      "Total training time: 1.22 seconds.\n",
      "Norm: 2.53, NNZs: 44, Bias: -0.301634, T: 1110930, Avg. loss: 0.260736\n",
      "Total training time: 1.33 seconds.\n",
      "Norm: 2.66, NNZs: 44, Bias: -0.350720, T: 1110930, Avg. loss: 1.097172Norm: 3.54, NNZs: 44, Bias: -0.185689, T: 1110930, Avg. loss: 1.565983\n",
      "Total training time: 1.34 seconds.\n",
      "\n",
      "Total training time: 1.35 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.4s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 2.44, NNZs: 44, Bias: -0.657794, T: 1110930, Avg. loss: 0.651359\n",
      "Total training time: 1.45 seconds.\n",
      "clf13 score: 0.682377086071255\n"
     ]
    }
   ],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf13.fit(X_train, y_train)\n",
    "print('clf13 score: {}'.format(clf13.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
